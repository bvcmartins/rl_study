{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import wandb\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Import comprehensive logging utilities (now all in one file)\n",
    "from logging_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ale_py\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to: 14127\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "SEED = 14127 \n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"Random seed set to: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, input_channels, action_size):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        \n",
    "        # Optimized architecture: 32→36→20 filters (vs original 32→64→64)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 36, kernel_size=4, stride=2, padding=1),  # Reduced from 64 to 36\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(36, 20, kernel_size=3, stride=1, padding=1),  # Reduced from 64 to 20\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Calculate correct feature size dynamically\n",
    "        self.feature_size = self._get_conv_output_size()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_size)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_output_size(self):\n",
    "        \"\"\"Calculate the output size of conv layers\"\"\"\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 2, 84, 84)  # 1 batch size, 2 channels, 84x84\n",
    "            conv_output = self.conv(dummy_input)\n",
    "            return conv_output.numel() // conv_output.size(0)  # Flatten size per sample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    import cv2\n",
    "    gray = np.mean(frame, axis=2).astype(np.uint8)\n",
    "    cropped = gray[34:194, :]\n",
    "    resized = cv2.resize(cropped, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "    return resized.astype(np.float32) / 255.0\n",
    "\n",
    "class FrameStack:\n",
    "    def __init__(self, num_frames=2):\n",
    "        self.num_frames = num_frames\n",
    "        self.frames = deque(maxlen=num_frames)\n",
    "    \n",
    "    def reset(self, frame):\n",
    "        processed_frame = preprocess_frame(frame)\n",
    "        for _ in range(self.num_frames):\n",
    "            self.frames.append(processed_frame)\n",
    "        return self.get_stacked()\n",
    "    \n",
    "    def step(self, frame):\n",
    "        processed_frame = preprocess_frame(frame)\n",
    "        self.frames.append(processed_frame)\n",
    "        return self.get_stacked()\n",
    "    \n",
    "    def get_stacked(self):\n",
    "        return np.stack(list(self.frames), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/bmartins/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbmartins\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# Global hyperparameters\n",
    "LEARNING_RATE = 0.0001\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 100000\n",
    "TARGET_UPDATE = 1000\n",
    "MEAN_REWARD_BOUND = 19.5  # Changed from MAX_REWARD_BOUND to MEAN_REWARD_BOUND\n",
    "FRAME_SKIP = 4\n",
    "INPUT_CHANNELS = 2\n",
    "FRAME_STACK = 2\n",
    "\n",
    "# Unified logging frequency - Enhanced monitoring configuration\n",
    "LOG_FREQ = 100  # Log comprehensive analysis every 100 episodes\n",
    "GRADIENT_LOG_FREQ = 500  # Log gradients every 500 steps\n",
    "\n",
    "# Architecture version for tracking\n",
    "ARCHITECTURE_NAME = \"CNN-DQN-v2\"\n",
    "ARCHITECTURE_TYPE = \"v2\"  # \"v2\" for 32-36-20 optimized architecture\n",
    "MODEL_FILTERS = \"32-36-20\"  # vs original \"32-64-64\"\n",
    "\n",
    "# Setup hyperparameters dictionary - SINGLE SOURCE OF TRUTH\n",
    "HYPERPARAMETERS = {\n",
    "    \"seed\": SEED,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"gamma\": GAMMA,\n",
    "    \"epsilon_start\": EPSILON_START,\n",
    "    \"epsilon_min\": EPSILON_MIN,\n",
    "    \"epsilon_decay\": EPSILON_DECAY,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"buffer_size\": BUFFER_SIZE,\n",
    "    \"target_update\": TARGET_UPDATE,\n",
    "    \"mean_reward_bound\": MEAN_REWARD_BOUND,  # Updated key name\n",
    "    \"frame_skip\": FRAME_SKIP,\n",
    "    \"input_channels\": INPUT_CHANNELS,\n",
    "    \"frame_stack\": FRAME_STACK,\n",
    "    \"architecture\": ARCHITECTURE_NAME,\n",
    "    \"model_filters\": MODEL_FILTERS,\n",
    "    \"architecture_type\": ARCHITECTURE_TYPE,\n",
    "    # Enhanced logging configuration\n",
    "    \"log_freq\": LOG_FREQ,\n",
    "    \"gradient_log_freq\": GRADIENT_LOG_FREQ,\n",
    "    \"comprehensive_monitoring\": True,\n",
    "    \"weight_bias_analysis\": True\n",
    "}\n",
    "\n",
    "# Set up directories for artifacts using relative paths\n",
    "RL_STUDY_DIR = Path(__file__).resolve().parent.parent.parent if '__file__' in globals() else Path.cwd().parent.parent\n",
    "MLFLOW_TRACKING_URI = RL_STUDY_DIR / 'mlruns'\n",
    "WANDB_DIR = RL_STUDY_DIR / 'wandb'\n",
    "BASE_ARTIFACTS_DIR = RL_STUDY_DIR / 'artifacts'\n",
    "\n",
    "# Configure MLflow to use local tracking\n",
    "mlflow.set_tracking_uri(f'file://{MLFLOW_TRACKING_URI}')\n",
    "\n",
    "# Configure wandb to use local directory\n",
    "os.environ['WANDB_DIR'] = str(WANDB_DIR)\n",
    "WANDB_DIR.mkdir(exist_ok=True)\n",
    "BASE_ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "MLFLOW_TRACKING_URI.mkdir(exist_ok=True)\n",
    "\n",
    "# Initialize wandb with unified project name\n",
    "load_dotenv(RL_STUDY_DIR / '.env')\n",
    "wandb_key = os.getenv('WANDB_KEY')\n",
    "wandb.login(key=wandb_key)\n",
    "\n",
    "wandb.init(\n",
    "    project=\"pong_dqn_training\",  # Unified project name same as v1\n",
    "    name=\"pong_cnn_dqn_v2_optimized\",\n",
    "    dir=str(WANDB_DIR),\n",
    "    config=HYPERPARAMETERS  # Use single source of truth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS now defined in cell-3 as single source of truth\n",
    "# All comprehensive logging functions imported from logging_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All weight visualization and analysis functions are now imported from logging_utils.py\n",
    "# This includes all enhanced weight/bias logging, comprehensive analysis, and visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn():\n",
    "    # End any active runs first\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n",
    "    \n",
    "    # MLflow experiment setup - unified with wandb name\n",
    "    mlflow.set_experiment(\"pong_dqn_training\")\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # Create artifacts directory using MLflow run ID and rl_study structure\n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        artifacts_dir = BASE_ARTIFACTS_DIR / f\"run_{run_id}\"\n",
    "        artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Artifacts will be saved to: {artifacts_dir}\")\n",
    "        \n",
    "        env = gym.make('PongNoFrameskip-v4')\n",
    "        env.reset(seed=SEED)  # Set seed for environment\n",
    "        \n",
    "        # Use global hyperparameters\n",
    "        action_size = env.action_space.n\n",
    "        lr = LEARNING_RATE\n",
    "        gamma = GAMMA\n",
    "        epsilon = EPSILON_START\n",
    "        epsilon_min = EPSILON_MIN\n",
    "        epsilon_decay = EPSILON_DECAY\n",
    "        batch_size = BATCH_SIZE\n",
    "        buffer_size = BUFFER_SIZE\n",
    "        target_update = TARGET_UPDATE\n",
    "        mean_reward_bound = MEAN_REWARD_BOUND  # Updated variable name\n",
    "        frame_skip = FRAME_SKIP\n",
    "        \n",
    "        # Log hyperparameters using external function\n",
    "        log_hyperparameters(str(artifacts_dir), HYPERPARAMETERS)\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        main_net = ConvDQN(input_channels=INPUT_CHANNELS, action_size=action_size).to(device)\n",
    "        target_net = ConvDQN(input_channels=INPUT_CHANNELS, action_size=action_size).to(device)\n",
    "        target_net.load_state_dict(main_net.state_dict())\n",
    "        \n",
    "        # Setup enhanced weight tracking using logging utils\n",
    "        setup_weight_tracking(main_net, log_freq=GRADIENT_LOG_FREQ, log_all=True)\n",
    "        \n",
    "        # Log model info using external function\n",
    "        total_params = log_model_info(main_net, device, ARCHITECTURE_NAME)\n",
    "        \n",
    "        optimizer = optim.Adam(main_net.parameters(), lr=lr)\n",
    "        buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        episode_rewards = []\n",
    "        mean_rewards = []\n",
    "        episode = 0\n",
    "        total_steps = 0\n",
    "        \n",
    "        while True:\n",
    "            state, _ = env.reset()\n",
    "            frame_stack = FrameStack(FRAME_STACK)\n",
    "            stacked_state = frame_stack.reset(state)\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            episode_losses = []\n",
    "            \n",
    "            while not done:\n",
    "                if random.random() > epsilon:\n",
    "                    with torch.no_grad():\n",
    "                        state_tensor = torch.FloatTensor(stacked_state).unsqueeze(0).to(device)\n",
    "                        q_values = main_net(state_tensor)\n",
    "                        action = q_values.max(1)[1].item()\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "                \n",
    "                step_reward = 0\n",
    "                for _ in range(frame_skip):\n",
    "                    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                    step_reward += reward\n",
    "                    if terminated or truncated:\n",
    "                        break\n",
    "                \n",
    "                done = terminated or truncated\n",
    "                next_stacked_state = frame_stack.step(next_state)\n",
    "                \n",
    "                buffer.push(stacked_state, action, step_reward, next_stacked_state, done)\n",
    "                stacked_state = next_stacked_state\n",
    "                total_reward += step_reward\n",
    "                step_count += 1\n",
    "                total_steps += 1\n",
    "                \n",
    "                if len(buffer) > batch_size:\n",
    "                    states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "                    \n",
    "                    states = torch.FloatTensor(states).to(device)\n",
    "                    actions = torch.LongTensor(actions).to(device)\n",
    "                    rewards = torch.FloatTensor(rewards).to(device)\n",
    "                    next_states = torch.FloatTensor(next_states).to(device)\n",
    "                    dones = torch.BoolTensor(dones).to(device)\n",
    "                    \n",
    "                    current_q_values = main_net(states).gather(1, actions.unsqueeze(1))\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        next_q_values = target_net(next_states).max(1)[0]\n",
    "                        target_q_values = rewards + (gamma * next_q_values * ~dones)\n",
    "                    \n",
    "                    loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n",
    "                    episode_losses.append(loss.item())\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(main_net.parameters(), max_norm=10)\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Enhanced gradient logging\n",
    "                    if total_steps % GRADIENT_LOG_FREQ == 0:\n",
    "                        log_training_step(loss.item(), episode, step_count)\n",
    "                        log_layer_wise_gradient_norms(main_net, total_steps)\n",
    "                        \n",
    "                        # Log weight and bias norms during training\n",
    "                        current_weight_norms = {}\n",
    "                        current_bias_norms = {}\n",
    "                        for name, param in main_net.named_parameters():\n",
    "                            if param.requires_grad:\n",
    "                                if 'bias' in name:\n",
    "                                    current_bias_norms[f\"bias_norms/{name}\"] = param.data.norm().item()\n",
    "                                else:\n",
    "                                    current_weight_norms[f\"weight_norms/{name}\"] = param.data.norm().item()\n",
    "                        \n",
    "                        wandb.log({\n",
    "                            **current_weight_norms,\n",
    "                            **current_bias_norms,\n",
    "                            \"training/total_steps\": total_steps,\n",
    "                            \"training/episode\": episode\n",
    "                        }, step=total_steps)\n",
    "                \n",
    "            if episode % target_update == 0:\n",
    "                target_net.load_state_dict(main_net.state_dict())\n",
    "            \n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n",
    "\n",
    "            episode_rewards.append(total_reward)\n",
    "            mean_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "            \n",
    "            # Log episode metrics with loss information - using total_steps\n",
    "            episode_loss_avg = np.mean(episode_losses) if episode_losses else 0.0\n",
    "            log_episode_metrics(total_reward, mean_reward, epsilon, len(buffer), step_count, episode, step=total_steps)\n",
    "            wandb.log({\n",
    "                \"episode/avg_loss\": episode_loss_avg,\n",
    "                \"episode/num_training_steps\": len(episode_losses)\n",
    "            }, step=total_steps)\n",
    "\n",
    "            # Debug line to check epsilon logging - using total_steps\n",
    "            wandb.log({\"debug/epsilon_direct\": epsilon}, step=total_steps)\n",
    "            \n",
    "            # Basic weight logging every LOG_FREQ episodes - using total_steps\n",
    "            if episode % LOG_FREQ == 0 and episode > 0:\n",
    "                wandb.log({\"weight_logging_episode\": episode}, step=total_steps)\n",
    "                log_weight_stats(model=main_net, episode=total_steps)\n",
    "            \n",
    "            # Check stopping condition: mean reward instead of max reward\n",
    "            if mean_reward >= mean_reward_bound:\n",
    "                max_reward_so_far = max(episode_rewards) if episode_rewards else 0\n",
    "                print(f\"Environment solved in {episode} episodes! Mean reward: {mean_reward:.2f} >= {mean_reward_bound}\")\n",
    "                print(f\"Max reward achieved: {max_reward_so_far:.2f}\")\n",
    "                wandb.log({\"solved_at_episode\": episode}, step=total_steps)\n",
    "                break\n",
    "            \n",
    "            # Save training plot every 1000 episodes\n",
    "            if episode > 0 and episode % 1000 == 0:\n",
    "                save_training_plot(str(artifacts_dir), episode_rewards, episode, mean_reward_bound, \" v2\")\n",
    "            \n",
    "            # Display plot every 100 episodes\n",
    "            if episode % 100 == 0 and episode > 0:\n",
    "                avg_reward = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)\n",
    "                max_reward_so_far = max(episode_rewards) if episode_rewards else 0\n",
    "                mean_rewards.append(avg_reward)\n",
    "                print(f\"Episode {episode}, Avg Reward: {avg_reward:.2f}, Mean Reward (100): {mean_reward:.2f}, Max Reward: {max_reward_so_far:.2f}, Epsilon: {epsilon:.3f}, Buffer: {len(buffer)}, Total Steps: {total_steps}\")\n",
    "                \n",
    "                display_training_plot(episode_rewards, mean_reward_bound, \" v2\")\n",
    "            \n",
    "            # Print progress every 10 episodes\n",
    "            elif episode % 10 == 0 and episode > 0:\n",
    "                avg_reward = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)\n",
    "                max_reward_so_far = max(episode_rewards) if episode_rewards else 0\n",
    "                print(f\"Episode {episode}, Avg Reward: {avg_reward:.2f}, Mean Reward (100): {mean_reward:.2f}, Max Reward: {max_reward_so_far:.2f}, Epsilon: {epsilon:.3f}, Buffer: {len(buffer)}, Total Steps: {total_steps}\")\n",
    "                \n",
    "                wandb.log({\"avg_reward_10\": avg_reward}, step=total_steps)\n",
    "\n",
    "            # Save checkpoint every 10000 episodes\n",
    "            if episode % 10000 == 0:\n",
    "                save_checkpoint(str(artifacts_dir), episode, main_net, optimizer, episode_rewards, epsilon, run_id, \"pong_dqn_cnn_v2\")\n",
    "\n",
    "            episode += 1\n",
    "        \n",
    "        # Log final results\n",
    "        final_model_path = log_final_results(main_net, episode, mean_reward, episode_rewards, str(artifacts_dir), run_id, \"final_pong_dqn_cnn_v2\")\n",
    "        \n",
    "        print(f\"Training completed! All artifacts saved in: {artifacts_dir}\")\n",
    "        print(f\"Total steps taken: {total_steps}\")\n",
    "        \n",
    "        env.close()\n",
    "        wandb.finish()\n",
    "        return main_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original frame shape: (210, 160, 3)\n",
      "Processed frame shape: (84, 84)\n",
      "Stacked frames shape: (2, 84, 84)\n",
      "Network input shape: torch.Size([1, 2, 84, 84])\n",
      "Q-values shape: torch.Size([1, 6])\n",
      "Q-values: [[ 0.00814844  0.04065464 -0.05447504 -0.00447786 -0.04875877 -0.02789652]]\n",
      "\n",
      "=== OPTIMIZED MODEL (v2) ===\n",
      "Architecture: CNN-DQN-v2\n",
      "Filter configuration: 32-36-20\n",
      "Total parameters: 1,056,686\n",
      "Conv parameters: 29,096 (2.8%)\n",
      "FC parameters: 1,027,590 (97.2%)\n",
      "Model size: 4.03 MB\n",
      "\n",
      "=== COMPARISON WITH ORIGINAL ===\n",
      "Original parameters: 3,354,278\n",
      "Optimized parameters: 1,056,686\n",
      "Parameter reduction: 2,297,592 (68.5%)\n",
      "Expected speedup: 3.2x\n"
     ]
    }
   ],
   "source": [
    "# Test optimized CNN architecture and compare with original\n",
    "env = gym.make('PongNoFrameskip-v4')\n",
    "state, _ = env.reset()\n",
    "\n",
    "print(f\"Original frame shape: {state.shape}\")\n",
    "\n",
    "processed = preprocess_frame(state)\n",
    "print(f\"Processed frame shape: {processed.shape}\")\n",
    "\n",
    "frame_stack = FrameStack(2)\n",
    "stacked = frame_stack.reset(state)\n",
    "print(f\"Stacked frames shape: {stacked.shape}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn_net_v2 = ConvDQN(input_channels=2, action_size=env.action_space.n).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    state_tensor = torch.FloatTensor(stacked).unsqueeze(0).to(device)\n",
    "    q_values = cnn_net_v2(state_tensor)\n",
    "    print(f\"Network input shape: {state_tensor.shape}\")\n",
    "    print(f\"Q-values shape: {q_values.shape}\")\n",
    "    print(f\"Q-values: {q_values.cpu().numpy()}\")\n",
    "\n",
    "# Calculate and compare parameters\n",
    "total_params_v2 = sum(p.numel() for p in cnn_net_v2.parameters())\n",
    "conv_params_v2 = sum(p.numel() for name, p in cnn_net_v2.named_parameters() if 'conv' in name)\n",
    "fc_params_v2 = sum(p.numel() for name, p in cnn_net_v2.named_parameters() if 'fc' in name)\n",
    "\n",
    "print(f\"\\n=== OPTIMIZED MODEL (v2) ===\")\n",
    "print(f\"Architecture: {ARCHITECTURE_NAME}\")\n",
    "print(f\"Filter configuration: {MODEL_FILTERS}\")\n",
    "print(f\"Total parameters: {total_params_v2:,}\")\n",
    "print(f\"Conv parameters: {conv_params_v2:,} ({conv_params_v2/total_params_v2*100:.1f}%)\")\n",
    "print(f\"FC parameters: {fc_params_v2:,} ({fc_params_v2/total_params_v2*100:.1f}%)\")\n",
    "print(f\"Model size: {total_params_v2 * 4 / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Original model comparison (from original training)\n",
    "original_params = 3354278  # From original training\n",
    "reduction_pct = (original_params - total_params_v2) / original_params * 100\n",
    "\n",
    "print(f\"\\n=== COMPARISON WITH ORIGINAL ===\")\n",
    "print(f\"Original parameters: {original_params:,}\")\n",
    "print(f\"Optimized parameters: {total_params_v2:,}\")\n",
    "print(f\"Parameter reduction: {original_params - total_params_v2:,} ({reduction_pct:.1f}%)\")\n",
    "print(f\"Expected speedup: {original_params / total_params_v2:.1f}x\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts will be saved to: /home/bmartins/dev/rl_study/artifacts/run_2491a30297df47e5bdbea98c99a5359c\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    }
   ],
   "source": [
    "# Ensure no active MLflow runs before training\n",
    "if mlflow.active_run():\n",
    "    print(\"Ending active MLflow run...\")\n",
    "    mlflow.end_run()\n",
    "\n",
    "# Train the CNN-based DQN\n",
    "trained_model = train_dqn()\n",
    "print(\"CNN-DQN Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_study_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
