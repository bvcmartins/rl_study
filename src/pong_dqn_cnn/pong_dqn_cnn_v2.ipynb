{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nfrom collections import deque\nimport gymnasium as gym\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nimport mlflow\nimport mlflow.pytorch\nimport wandb\nimport os\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\n# Import comprehensive logging utilities (now all in one file)\nfrom logging_utils import *"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ale_py\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, input_channels, action_size):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        \n",
    "        # Optimized architecture: 32→36→20 filters (vs original 32→64→64)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 36, kernel_size=4, stride=2, padding=1),  # Reduced from 64 to 36\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(36, 20, kernel_size=3, stride=1, padding=1),  # Reduced from 64 to 20\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Calculate correct feature size dynamically\n",
    "        self.feature_size = self._get_conv_output_size()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_size)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_output_size(self):\n",
    "        \"\"\"Calculate the output size of conv layers\"\"\"\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 2, 84, 84)  # 1 batch size, 2 channels, 84x84\n",
    "            conv_output = self.conv(dummy_input)\n",
    "            return conv_output.numel() // conv_output.size(0)  # Flatten size per sample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    import cv2\n",
    "    gray = np.mean(frame, axis=2).astype(np.uint8)\n",
    "    cropped = gray[34:194, :]\n",
    "    resized = cv2.resize(cropped, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "    return resized.astype(np.float32) / 255.0\n",
    "\n",
    "class FrameStack:\n",
    "    def __init__(self, num_frames=2):\n",
    "        self.num_frames = num_frames\n",
    "        self.frames = deque(maxlen=num_frames)\n",
    "    \n",
    "    def reset(self, frame):\n",
    "        processed_frame = preprocess_frame(frame)\n",
    "        for _ in range(self.num_frames):\n",
    "            self.frames.append(processed_frame)\n",
    "        return self.get_stacked()\n",
    "    \n",
    "    def step(self, frame):\n",
    "        processed_frame = preprocess_frame(frame)\n",
    "        self.frames.append(processed_frame)\n",
    "        return self.get_stacked()\n",
    "    \n",
    "    def get_stacked(self):\n",
    "        return np.stack(list(self.frames), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Global hyperparameters\nLEARNING_RATE = 0.0001\nGAMMA = 0.99\nEPSILON_START = 1.0\nEPSILON_MIN = 0.01\nEPSILON_DECAY = 0.995\nBATCH_SIZE = 32\nBUFFER_SIZE = 100000\nTARGET_UPDATE = 1000\nMAX_REWARD_BOUND = 19.5  # Changed from MEAN_REWARD_BOUND to MAX_REWARD_BOUND\nFRAME_SKIP = 4\nINPUT_CHANNELS = 2\nFRAME_STACK = 2\n\n# Unified logging frequency - Enhanced monitoring configuration\nLOG_FREQ = 100  # Log comprehensive analysis every 100 episodes\nGRADIENT_LOG_FREQ = 500  # Log gradients every 500 steps\n\n# Architecture version for tracking\nARCHITECTURE_NAME = \"CNN-DQN-v2\"\nARCHITECTURE_TYPE = \"v2\"  # \"v2\" for 32-36-20 optimized architecture\nMODEL_FILTERS = \"32-36-20\"  # vs original \"32-64-64\"\n\n# Setup hyperparameters dictionary - SINGLE SOURCE OF TRUTH\nHYPERPARAMETERS = {\n    \"learning_rate\": LEARNING_RATE,\n    \"gamma\": GAMMA,\n    \"epsilon_start\": EPSILON_START,\n    \"epsilon_min\": EPSILON_MIN,\n    \"epsilon_decay\": EPSILON_DECAY,\n    \"batch_size\": BATCH_SIZE,\n    \"buffer_size\": BUFFER_SIZE,\n    \"target_update\": TARGET_UPDATE,\n    \"max_reward_bound\": MAX_REWARD_BOUND,\n    \"frame_skip\": FRAME_SKIP,\n    \"input_channels\": INPUT_CHANNELS,\n    \"frame_stack\": FRAME_STACK,\n    \"architecture\": ARCHITECTURE_NAME,\n    \"model_filters\": MODEL_FILTERS,\n    \"architecture_type\": ARCHITECTURE_TYPE,\n    # Enhanced logging configuration\n    \"log_freq\": LOG_FREQ,\n    \"gradient_log_freq\": GRADIENT_LOG_FREQ,\n    \"comprehensive_monitoring\": True,\n    \"weight_bias_analysis\": True\n}\n\n# Set up directories for artifacts using relative paths\nRL_STUDY_DIR = Path(__file__).resolve().parent.parent.parent if '__file__' in globals() else Path.cwd().parent.parent\nMLFLOW_TRACKING_URI = RL_STUDY_DIR / 'mlruns'\nWANDB_DIR = RL_STUDY_DIR / 'wandb'\nBASE_ARTIFACTS_DIR = RL_STUDY_DIR / 'artifacts'\n\n# Configure MLflow to use local tracking\nmlflow.set_tracking_uri(f'file://{MLFLOW_TRACKING_URI}')\n\n# Configure wandb to use local directory\nos.environ['WANDB_DIR'] = str(WANDB_DIR)\nWANDB_DIR.mkdir(exist_ok=True)\nBASE_ARTIFACTS_DIR.mkdir(exist_ok=True)\nMLFLOW_TRACKING_URI.mkdir(exist_ok=True)\n\n# Initialize wandb with unified project name\nload_dotenv(RL_STUDY_DIR / '.env')\nwandb_key = os.getenv('WANDB_KEY')\nwandb.login(key=wandb_key)\n\nwandb.init(\n    project=\"pong_dqn_training\",  # Unified project name same as v1\n    name=\"pong_cnn_dqn_v2_optimized\",\n    dir=str(WANDB_DIR),\n    config=HYPERPARAMETERS  # Use single source of truth\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# HYPERPARAMETERS now defined in cell-3 as single source of truth\n# All comprehensive logging functions imported from logging_utils.py"
  },
  {
   "cell_type": "code",
   "source": "# All weight visualization and analysis functions are now imported from logging_utils.py\n# This includes all enhanced weight/bias logging, comprehensive analysis, and visualization functions",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_dqn():\n    # End any active runs first\n    if mlflow.active_run():\n        mlflow.end_run()\n    \n    # MLflow experiment setup - unified with wandb name\n    mlflow.set_experiment(\"pong_dqn_training\")\n    \n    with mlflow.start_run():\n        # Create artifacts directory using MLflow run ID and rl_study structure\n        run_id = mlflow.active_run().info.run_id\n        artifacts_dir = BASE_ARTIFACTS_DIR / f\"run_{run_id}\"\n        artifacts_dir.mkdir(parents=True, exist_ok=True)\n        print(f\"Artifacts will be saved to: {artifacts_dir}\")\n        \n        env = gym.make('PongNoFrameskip-v4')\n        \n        # Use global hyperparameters\n        action_size = env.action_space.n\n        lr = LEARNING_RATE\n        gamma = GAMMA\n        epsilon = EPSILON_START\n        epsilon_min = EPSILON_MIN\n        epsilon_decay = EPSILON_DECAY\n        batch_size = BATCH_SIZE\n        buffer_size = BUFFER_SIZE\n        target_update = TARGET_UPDATE\n        max_reward_bound = MAX_REWARD_BOUND  # Updated variable name\n        frame_skip = FRAME_SKIP\n        \n        # Log hyperparameters using external function\n        log_hyperparameters(str(artifacts_dir), HYPERPARAMETERS)\n        \n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Using device: {device}\")\n        \n        main_net = ConvDQN(input_channels=INPUT_CHANNELS, action_size=action_size).to(device)\n        target_net = ConvDQN(input_channels=INPUT_CHANNELS, action_size=action_size).to(device)\n        target_net.load_state_dict(main_net.state_dict())\n        \n        # Setup enhanced weight tracking using logging utils\n        setup_weight_tracking(main_net, log_freq=GRADIENT_LOG_FREQ, log_all=True)\n        \n        # Log model info using external function\n        total_params = log_model_info(main_net, device, ARCHITECTURE_NAME)\n        \n        optimizer = optim.Adam(main_net.parameters(), lr=lr)\n        buffer = ReplayBuffer(buffer_size)\n        \n        episode_rewards = []\n        mean_rewards = []\n        episode = 0\n        total_steps = 0\n        \n        while True:\n            state, _ = env.reset()\n            frame_stack = FrameStack(FRAME_STACK)\n            stacked_state = frame_stack.reset(state)\n            total_reward = 0\n            done = False\n            step_count = 0\n            episode_losses = []\n            \n            while not done:\n                if random.random() > epsilon:\n                    with torch.no_grad():\n                        state_tensor = torch.FloatTensor(stacked_state).unsqueeze(0).to(device)\n                        q_values = main_net(state_tensor)\n                        action = q_values.max(1)[1].item()\n                else:\n                    action = env.action_space.sample()\n                \n                step_reward = 0\n                for _ in range(frame_skip):\n                    next_state, reward, terminated, truncated, _ = env.step(action)\n                    step_reward += reward\n                    if terminated or truncated:\n                        break\n                \n                done = terminated or truncated\n                next_stacked_state = frame_stack.step(next_state)\n                \n                buffer.push(stacked_state, action, step_reward, next_stacked_state, done)\n                stacked_state = next_stacked_state\n                total_reward += step_reward\n                step_count += 1\n                total_steps += 1\n                \n                if len(buffer) > batch_size:\n                    states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n                    \n                    states = torch.FloatTensor(states).to(device)\n                    actions = torch.LongTensor(actions).to(device)\n                    rewards = torch.FloatTensor(rewards).to(device)\n                    next_states = torch.FloatTensor(next_states).to(device)\n                    dones = torch.BoolTensor(dones).to(device)\n                    \n                    current_q_values = main_net(states).gather(1, actions.unsqueeze(1))\n                    \n                    with torch.no_grad():\n                        next_q_values = target_net(next_states).max(1)[0]\n                        target_q_values = rewards + (gamma * next_q_values * ~dones)\n                    \n                    loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n                    episode_losses.append(loss.item())\n                    \n                    optimizer.zero_grad()\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(main_net.parameters(), max_norm=10)\n                    optimizer.step()\n                    \n                    # Enhanced gradient logging\n                    if total_steps % GRADIENT_LOG_FREQ == 0:\n                        log_training_step(loss.item(), episode, step_count)\n                        log_layer_wise_gradient_norms(main_net, total_steps)\n                        \n                        # Log weight and bias norms during training\n                        current_weight_norms = {}\n                        current_bias_norms = {}\n                        for name, param in main_net.named_parameters():\n                            if param.requires_grad:\n                                if 'bias' in name:\n                                    current_bias_norms[f\"bias_norms/{name}\"] = param.data.norm().item()\n                                else:\n                                    current_weight_norms[f\"weight_norms/{name}\"] = param.data.norm().item()\n                        \n                        wandb.log({\n                            **current_weight_norms,\n                            **current_bias_norms,\n                            \"training/total_steps\": total_steps,\n                            \"training/episode\": episode\n                        }, step=total_steps)\n                \n            if episode % target_update == 0:\n                target_net.load_state_dict(main_net.state_dict())\n            \n            if epsilon > epsilon_min:\n                epsilon *= epsilon_decay\n\n            episode_rewards.append(total_reward)\n            mean_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n            \n            # Log episode metrics with loss information - using total_steps\n            episode_loss_avg = np.mean(episode_losses) if episode_losses else 0.0\n            log_episode_metrics(total_reward, mean_reward, epsilon, len(buffer), step_count, episode, step=total_steps)\n            wandb.log({\n                \"episode/avg_loss\": episode_loss_avg,\n                \"episode/num_training_steps\": len(episode_losses)\n            }, step=total_steps)\n\n            # Debug line to check epsilon logging - using total_steps\n            wandb.log({\"debug/epsilon_direct\": epsilon}, step=total_steps)\n            \n            # Basic weight logging every LOG_FREQ episodes - using total_steps\n            if episode % LOG_FREQ == 0 and episode > 0:\n                wandb.log({\"weight_logging_episode\": episode}, step=total_steps)\n                log_weight_stats(model=main_net, episode=total_steps)\n            \n            # Check stopping condition: max reward instead of mean reward\n            max_reward = max(episode_rewards) if episode_rewards else 0\n            if max_reward >= max_reward_bound:\n                print(f\"Environment solved in {episode} episodes! Max reward: {max_reward:.2f} >= {max_reward_bound}\")\n                print(f\"Current mean reward (last 100): {mean_reward:.2f}\")\n                wandb.log({\"solved_at_episode\": episode}, step=total_steps)\n                break\n            \n            # Save training plot every 1000 episodes\n            if episode > 0 and episode % 1000 == 0:\n                save_training_plot(str(artifacts_dir), episode_rewards, episode, max_reward_bound, \" v2\")\n            \n            # Display plot every 100 episodes\n            if episode % 100 == 0 and episode > 0:\n                avg_reward = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)\n                max_reward_so_far = max(episode_rewards) if episode_rewards else 0\n                mean_rewards.append(avg_reward)\n                print(f\"Episode {episode}, Avg Reward: {avg_reward:.2f}, Max Reward: {max_reward_so_far:.2f}, Epsilon: {epsilon:.3f}, Buffer: {len(buffer)}, Total Steps: {total_steps}\")\n                \n                display_training_plot(episode_rewards, max_reward_bound, \" v2\")\n            \n            # Print progress every 10 episodes\n            elif episode % 10 == 0 and episode > 0:\n                avg_reward = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)\n                max_reward_so_far = max(episode_rewards) if episode_rewards else 0\n                print(f\"Episode {episode}, Avg Reward: {avg_reward:.2f}, Max Reward: {max_reward_so_far:.2f}, Epsilon: {epsilon:.3f}, Buffer: {len(buffer)}, Total Steps: {total_steps}\")\n                \n                wandb.log({\"avg_reward_10\": avg_reward}, step=total_steps)\n\n            # Save checkpoint every 10000 episodes\n            if episode % 10000 == 0:\n                save_checkpoint(str(artifacts_dir), episode, main_net, optimizer, episode_rewards, epsilon, run_id, \"pong_dqn_cnn_v2\")\n\n            episode += 1\n        \n        # Log final results\n        final_model_path = log_final_results(main_net, episode, mean_reward, episode_rewards, str(artifacts_dir), run_id, \"final_pong_dqn_cnn_v2\")\n        \n        print(f\"Training completed! All artifacts saved in: {artifacts_dir}\")\n        print(f\"Total steps taken: {total_steps}\")\n        \n        env.close()\n        wandb.finish()\n        return main_net"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test optimized CNN architecture and compare with original\nenv = gym.make('PongNoFrameskip-v4')\nstate, _ = env.reset()\n\nprint(f\"Original frame shape: {state.shape}\")\n\nprocessed = preprocess_frame(state)\nprint(f\"Processed frame shape: {processed.shape}\")\n\nframe_stack = FrameStack(2)\nstacked = frame_stack.reset(state)\nprint(f\"Stacked frames shape: {stacked.shape}\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncnn_net_v2 = ConvDQN(input_channels=2, action_size=env.action_space.n).to(device)\n\nwith torch.no_grad():\n    state_tensor = torch.FloatTensor(stacked).unsqueeze(0).to(device)\n    q_values = cnn_net_v2(state_tensor)\n    print(f\"Network input shape: {state_tensor.shape}\")\n    print(f\"Q-values shape: {q_values.shape}\")\n    print(f\"Q-values: {q_values.cpu().numpy()}\")\n\n# Calculate and compare parameters\ntotal_params_v2 = sum(p.numel() for p in cnn_net_v2.parameters())\nconv_params_v2 = sum(p.numel() for name, p in cnn_net_v2.named_parameters() if 'conv' in name)\nfc_params_v2 = sum(p.numel() for name, p in cnn_net_v2.named_parameters() if 'fc' in name)\n\nprint(f\"\\n=== OPTIMIZED MODEL (v2) ===\")\nprint(f\"Architecture: {ARCHITECTURE_NAME}\")\nprint(f\"Filter configuration: {MODEL_FILTERS}\")\nprint(f\"Total parameters: {total_params_v2:,}\")\nprint(f\"Conv parameters: {conv_params_v2:,} ({conv_params_v2/total_params_v2*100:.1f}%)\")\nprint(f\"FC parameters: {fc_params_v2:,} ({fc_params_v2/total_params_v2*100:.1f}%)\")\nprint(f\"Model size: {total_params_v2 * 4 / (1024*1024):.2f} MB\")\n\n# Original model comparison (from original training)\noriginal_params = 3354278  # From original training\nreduction_pct = (original_params - total_params_v2) / original_params * 100\n\nprint(f\"\\n=== COMPARISON WITH ORIGINAL ===\")\nprint(f\"Original parameters: {original_params:,}\")\nprint(f\"Optimized parameters: {total_params_v2:,}\")\nprint(f\"Parameter reduction: {original_params - total_params_v2:,} ({reduction_pct:.1f}%)\")\nprint(f\"Expected speedup: {original_params / total_params_v2:.1f}x\")\n\nenv.close()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ensure no active MLflow runs before training\nif mlflow.active_run():\n    print(\"Ending active MLflow run...\")\n    mlflow.end_run()\n\n# Train the CNN-based DQN\ntrained_model = train_dqn()\nprint(\"CNN-DQN Training completed!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_study_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}