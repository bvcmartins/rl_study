{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmartins/anaconda3/envs/rl_study/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up environment\nimport ale_py\n\ngym.register_envs(ale_py)  # unnecessary but helpful for IDEs\nenv_name = 'Pong-v0'\nenv = gym.make(env_name, render_mode='rgb_array')\nstate_size = (84, 84, 1)  # Single frame for simplicity\naction_size = env.action_space.n\nbuffer_size = 100000\nbatch_size = 64\ngamma = 0.99  # Discount factor\nepsilon = 1.0  # Exploration rate\nepsilon_min = 0.01  # Minimum exploration rate\nepsilon_decay = 0.995  # Decay rate for epsilon"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_preprocessing(observation_frame):\n",
    "    # Crop the frame.\n",
    "    observation_frame = observation_frame[35:195]\n",
    "    # Downsample the frame by a factor of 2.\n",
    "    observation_frame = observation_frame[::2, ::2, 0]\n",
    "    # Remove the background and apply other enhancements.\n",
    "    observation_frame[observation_frame == 144] = 0  # Erase the background (type 1).\n",
    "    observation_frame[observation_frame == 109] = 0  # Erase the background (type 2).\n",
    "    observation_frame[observation_frame != 0] = 1  # Set the items (rackets, ball) to 1.\n",
    "    # Return the preprocessed frame as a 1D floating-point array.\n",
    "    return observation_frame.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Preprocess frame\ndef preprocess_frame(frame):\n    \"\"\"\n    Convert frame to grayscale, crop, resize to 84x84, and normalize.\n    Returns a (84, 84) numpy array with values in [0, 1].\n    \"\"\"\n    # Convert to grayscale\n    frame = np.mean(frame, axis=2).astype(np.uint8)\n    \n    # Crop the screen to focus on the playing area (remove score and borders)\n    frame = frame[34:194]  # Height: 210 -> 160\n    \n    # Resize to 84x84 using cv2\n    frame = cv2.resize(frame, (84, 84), interpolation=cv2.INTER_AREA)\n    \n    # Normalize pixel values to [0, 1]\n    frame = frame.astype(np.float32) / 255.0\n    \n    return frame"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# play the game using a random policy and render the frames in the notebook\n",
    "obs, info = env.reset() \n",
    "obs = preprocess_frame(obs)\n",
    "done = False\n",
    "total_reward = 0\n",
    "    \n",
    "while not done:\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    plt.imshow(obs) \n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n",
    "    obs = preprocess_frame(obs)\n",
    "    total_reward += reward\n",
    "        \n",
    "    if terminated or truncated:\n",
    "        done = True\n",
    "            \n",
    "time.sleep(1)\n",
    "\n",
    "env.close() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play the game with the UI using a random policy \n",
    "def play_random_policy(env, num_episodes=10):\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()  # Random action\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        env.render()  # Render the environment\n",
    "        time.sleep(1)\n",
    "        env.close()\n",
    "\n",
    "play_random_policy(env, num_episodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build neural network model\nclass DQN(nn.Module):\n    def __init__(self, input_shape, action_size):\n        super(DQN, self).__init__()\n        self.input_shape = input_shape  # Expected: (batch_size, channels, height, width)\n        \n        # Convolutional layers - designed for (1, 84, 84) input\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=8, stride=4),  # (32, 20, 20)\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),  # (64, 9, 9)\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),  # (64, 7, 7)\n            nn.ReLU(),\n            nn.Flatten()\n        )\n        \n        # Calculate the output size of conv layers\n        self.conv_output_size = self._get_conv_output()\n        \n        # Fully connected layers\n        self.fc_layers = nn.Sequential(\n            nn.Linear(self.conv_output_size, 512),\n            nn.ReLU(),\n            nn.Linear(512, action_size)\n        )\n    \n    def _get_conv_output(self):\n        \"\"\"\n        Helper method to get the output size of the convolutional layers.\n        \"\"\"\n        # Create dummy input with shape (batch_size=1, channels=1, height=84, width=84)\n        dummy_input = torch.randn(1, 1, 84, 84)\n        output = self.conv_layers(dummy_input)\n        return output.size(1)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the network.\n        Input: (batch_size, 84, 84) - single channel grayscale frames\n        Returns Q-values for each action.\n        \"\"\"\n        # Add channel dimension if needed: (batch_size, 84, 84) -> (batch_size, 1, 84, 84)\n        if len(x.shape) == 3:\n            x = x.unsqueeze(1)\n        \n        # Input is already normalized in preprocessing, no need to normalize again\n        x = self.conv_layers(x)\n        x = self.fc_layers(x)\n        return x"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "obs, _ = env.reset()\nprint(\"Original obs shape:\", obs.shape)\nstate = preprocess_frame(obs)\nprint(\"Processed state shape:\", state.shape)\nstate_tensor = torch.tensor(np.array([state]), dtype=torch.float32)\nprint(\"State tensor shape:\", state_tensor.shape)\naction_size = env.action_space.n\nmodel = DQN((1, 84, 84), action_size).float()  # Use correct input shape\nq_values = model(state_tensor)\nprint(\"Q-values shape:\", q_values.shape)\naction = 0\nstep = env.step(action)\nprint(\"Step info:\", step[4])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((210, 160, 3), torch.Size([1, 84, 84]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape, state_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n\n\nclass ExperienceBuffer:\n    def __init__(self, capacity):\n        self.buffer = collections.deque(maxlen=capacity)\n\n    def __len__(self):\n        return len(self.buffer)\n\n    def append(self, experience):\n        self.buffer.append(experience)\n\n    def sample(self, batch_size):\n        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n               np.array(dones, dtype=np.uint8), np.array(next_states)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "GAMMA = 0.99\nclass Agent:\n    def __init__(self, env, exp_buffer):\n        self.env = env\n        self.exp_buffer = exp_buffer\n        self._reset()\n\n    def _reset(self):\n        self.state, info = self.env.reset()\n        self.total_reward = 0.0\n\n    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n        done_reward = None\n\n        if np.random.random() < epsilon:\n            action = self.env.action_space.sample()  # Use self.env instead of global env\n        else:\n            state_a = np.array([preprocess_frame(self.state)], copy=False)\n            state_v = torch.tensor(state_a, dtype=torch.float32).to(device)\n            q_vals_v = net(state_v)\n            _, act_v = torch.max(q_vals_v, dim=1)\n            action = int(act_v.item())\n\n        # do step in the environment\n        new_state, reward, is_done, _, _ = self.env.step(action)\n        self.total_reward += reward\n\n        exp = Experience(preprocess_frame(self.state), action, reward, is_done, preprocess_frame(new_state))\n        self.exp_buffer.append(exp)\n        self.state = new_state\n        if is_done:\n            done_reward = self.total_reward\n            self._reset()\n        return done_reward\n\n\ndef calc_loss(batch, net, tgt_net, device=\"cpu\"):\n    states, actions, rewards, dones, next_states = batch\n\n    states_v = torch.tensor(states, dtype=torch.float32).to(device)\n    next_states_v = torch.tensor(next_states, dtype=torch.float32).to(device)\n    actions_v = torch.tensor(actions, dtype=torch.long).to(device)\n    rewards_v = torch.tensor(rewards, dtype=torch.float32).to(device)\n    done_mask = torch.tensor(dones, dtype=torch.bool).to(device)  # Use bool instead of ByteTensor\n\n    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n    next_state_values = tgt_net(next_states_v).max(1)[0]\n    next_state_values[done_mask] = 0.0\n    next_state_values = next_state_values.detach()\n\n    expected_state_action_values = next_state_values * GAMMA + rewards_v\n    return nn.MSELoss()(state_action_values, expected_state_action_values)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def play_policy(env, net, device, sleep_time=0.05):\n    obs, info = env.reset() \n    done = False\n    total_reward = 0\n        \n    while not done:\n        state_a = np.array([preprocess_frame(obs)], copy=False)\n        state_v = torch.tensor(state_a).to(device)\n        q_vals_v = net(state_v)\n        _, act_v = torch.max(q_vals_v, dim=1)\n        action = int(act_v.item())\n        \n        # do step in the environment\n        obs, reward, terminated, truncated, info = env.step(action)\n        plt.imshow(obs) \n        plt.show()\n        clear_output(wait=True)\n        total_reward += reward\n            \n        if terminated or truncated:\n            done = True\n                \n        time.sleep(sleep_time)\n    env.close() \n    return total_reward"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "MEAN_REWARD_BOUND = 19.5\n\nGAMMA = 0.99\nBATCH_SIZE = 32\nREPLAY_SIZE = 50000\nLEARNING_RATE = 1e-4  # Reduced from 1e-2 for more stable training\nSYNC_TARGET_FRAMES = 1000\nREPLAY_START_SIZE = 10000\n\nEPSILON_DECAY_LAST_FRAME = 10**6\nEPSILON_START = 1.0\nEPSILON_FINAL = 0.02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training loop setup\n%matplotlib inline\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Initialize networks and optimizer with correct input shape\nnet = DQN((1, 84, 84), env.action_space.n).to(device)\ntgt_net = DQN((1, 84, 84), env.action_space.n).to(device)\noptimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n\n# Initialize buffers and tracking variables\nbuffer = ExperienceBuffer(REPLAY_SIZE)\nagent = Agent(env, buffer)\ntotal_rewards = []\nframe_idx = 0\nts_frame = 0\nts = time.time()\nbest_mean_reward = None\nreward_history = []\n\nwhile True:\n    frame_idx += 1\n    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n    reward = agent.play_step(net, epsilon, device=device)\n    \n    if reward is not None:\n        total_rewards.append(reward)\n        speed = (frame_idx - ts_frame) / (time.time() - ts)\n        ts_frame = frame_idx\n        ts = time.time()\n        mean_reward = np.mean(total_rewards[-100:])\n        reward_history.append(mean_reward)\n        \n        # Print progress\n        print(f\"Frame {frame_idx}: games={len(total_rewards)}, \"\n              f\"mean reward={mean_reward:.3f}, eps={epsilon:.2f}, \"\n              f\"speed={speed:.2f} f/s\")\n        \n        # Check for best reward\n        if best_mean_reward is None or best_mean_reward < mean_reward:\n            if best_mean_reward is not None:\n                print(f\"Best mean reward updated {best_mean_reward:.3f} -> {mean_reward:.3f}\")\n            best_mean_reward = mean_reward\n            \n        # Check if solved\n        if mean_reward > MEAN_REWARD_BOUND:\n            print(f\"Solved in {frame_idx} frames!\")\n            break\n\n    # Skip until we have enough data\n    if len(buffer) < REPLAY_START_SIZE:\n        continue\n\n    # Periodic target network update and visualization\n    if frame_idx % SYNC_TARGET_FRAMES == 0:\n        tgt_net.load_state_dict(net.state_dict())\n        \n        # Update plot\n        clear_output(wait=True)\n        plt.figure(figsize=(10, 5))\n        plt.plot(reward_history)\n        plt.title('Training Progress')\n        plt.xlabel('Episodes')\n        plt.ylabel('Mean Reward')\n        plt.grid(True)\n        plt.show()\n    \n    # Training step\n    optimizer.zero_grad()\n    batch = buffer.sample(BATCH_SIZE)\n    loss_t = calc_loss(batch, net, tgt_net, device=device)\n    loss_t.backward()\n    optimizer.step()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}