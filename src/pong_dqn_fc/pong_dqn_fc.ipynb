{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b57f1bb",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nfrom collections import deque\nimport gymnasium as gym\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nimport mlflow\nimport mlflow.pytorch\nimport wandb\nimport os\nfrom dotenv import load_dotenv\nfrom pathlib import Path\nfrom logging_utils import *"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04afd267",
   "metadata": {},
   "outputs": [],
   "source": "# Global hyperparameters\nLEARNING_RATE = 0.001\nGAMMA = 0.99\nEPSILON_START = 1.0\nEPSILON_MIN = 0.01\nEPSILON_DECAY = 0.99\nBATCH_SIZE = 32\nBUFFER_SIZE = 50000\nTARGET_UPDATE = 1000\nMEAN_REWARD_BOUND = 19.5\nFRAME_SKIP = 1  # FC version uses regular Pong-v4\nINPUT_CHANNELS = 2\nFRAME_STACK = 2\n\n# Initialize wandb with global parameters\nload_dotenv('/home/bmartins/dev/rl_study/.env')\nwandb_key = os.getenv('WANDB_KEY')\nwandb.login(key=wandb_key)\n\nwandb.init(\n    project=\"pong-dqn-training\",\n    name=\"pong_fc_dqn\",\n    config={\n        \"learning_rate\": LEARNING_RATE,\n        \"gamma\": GAMMA,\n        \"epsilon_start\": EPSILON_START,\n        \"epsilon_min\": EPSILON_MIN,\n        \"epsilon_decay\": EPSILON_DECAY,\n        \"batch_size\": BATCH_SIZE,\n        \"buffer_size\": BUFFER_SIZE,\n        \"target_update\": TARGET_UPDATE,\n        \"mean_reward_bound\": MEAN_REWARD_BOUND,\n        \"frame_skip\": FRAME_SKIP,\n        \"architecture\": \"FC-DQN\",\n        \"input_channels\": INPUT_CHANNELS,\n        \"frame_stack\": FRAME_STACK\n    }\n)"
  },
  {
   "cell_type": "code",
   "id": "xpzavg5viii",
   "source": "# Logging functions\ndef log_hyperparameters(artifacts_dir):\n    \"\"\"Log hyperparameters to MLflow\"\"\"\n    mlflow.log_params({\n        \"learning_rate\": LEARNING_RATE,\n        \"gamma\": GAMMA,\n        \"epsilon_start\": EPSILON_START,\n        \"epsilon_min\": EPSILON_MIN,\n        \"epsilon_decay\": EPSILON_DECAY,\n        \"batch_size\": BATCH_SIZE,\n        \"buffer_size\": BUFFER_SIZE,\n        \"target_update\": TARGET_UPDATE,\n        \"mean_reward_bound\": MEAN_REWARD_BOUND,\n        \"frame_skip\": FRAME_SKIP,\n        \"input_channels\": INPUT_CHANNELS,\n        \"frame_stack\": FRAME_STACK,\n        \"artifacts_dir\": artifacts_dir\n    })\n\ndef log_model_info(model, device):\n    \"\"\"Log model information to both MLflow and wandb\"\"\"\n    total_params = sum(p.numel() for p in model.parameters())\n    mlflow.log_param(\"total_parameters\", total_params)\n    mlflow.log_param(\"device\", str(device))\n    wandb.log({\"total_parameters\": total_params, \"device\": str(device)})\n    return total_params\n\ndef log_training_step(loss, episode, step_count):\n    \"\"\"Log training step metrics\"\"\"\n    mlflow.log_metric(\"loss\", loss, step=episode * 10000 + step_count)\n    wandb.log({\"loss\": loss, \"step\": episode * 10000 + step_count})\n\ndef log_episode_metrics(episode_reward, mean_reward, epsilon, buffer_size, episode_length, episode):\n    \"\"\"Log episode metrics to both MLflow and wandb\"\"\"\n    metrics = {\n        \"episode_reward\": episode_reward,\n        \"mean_reward_100\": mean_reward,\n        \"epsilon\": epsilon,\n        \"buffer_size\": buffer_size,\n        \"episode_length\": episode_length,\n        \"episode\": episode\n    }\n    \n    mlflow.log_metrics({\n        \"episode_reward\": episode_reward,\n        \"mean_reward_100\": mean_reward,\n        \"epsilon\": epsilon,\n        \"buffer_size\": buffer_size,\n        \"episode_length\": episode_length\n    }, step=episode)\n    \n    wandb.log(metrics)\n\ndef log_10_episode_average(avg_reward, episode):\n    \"\"\"Log 10-episode average reward\"\"\"\n    mlflow.log_metric(\"avg_reward_10\", avg_reward, step=episode)\n    wandb.log({\"avg_reward_10\": avg_reward})\n\ndef log_solved_episode(episode):\n    \"\"\"Log when environment is solved\"\"\"\n    mlflow.log_metric(\"solved_at_episode\", episode)\n    wandb.log({\"solved_at_episode\": episode})\n\ndef save_checkpoint(artifacts_dir, episode, model, optimizer, episode_rewards, epsilon, run_id):\n    \"\"\"Save model checkpoint and log to tracking systems\"\"\"\n    checkpoint_path = os.path.join(artifacts_dir, f'pong_dqn_fc_checkpoint_ep{episode}.pth')\n    torch.save({\n        'episode': episode,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'avg_reward': np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards),\n        'epsilon': epsilon,\n        'episode_rewards': episode_rewards,\n        'run_id': run_id\n    }, checkpoint_path)\n    \n    # Log checkpoint to MLflow and wandb\n    mlflow.log_artifact(checkpoint_path)\n    wandb.save(checkpoint_path)\n    print(f\"Checkpoint saved at episode {episode}: {checkpoint_path}\")\n    return checkpoint_path\n\ndef save_training_plot(artifacts_dir, episode_rewards, episode, mean_reward_bound):\n    \"\"\"Save training plot to file and log to tracking systems\"\"\"\n    if len(episode_rewards) <= 1:\n        return\n    \n    # Calculate mean rewards for plotting (using 10-episode averages)\n    plot_episodes = []\n    plot_rewards = []\n    for i in range(10, len(episode_rewards) + 1, 10):\n        plot_episodes.append(i)\n        plot_rewards.append(np.mean(episode_rewards[i-10:i]))\n    \n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(plot_episodes, plot_rewards, 'b-', linewidth=2, marker='o')\n    plt.title('FC-DQN Pong Training Progress')\n    plt.xlabel('Episode')\n    plt.ylabel('Mean Reward (last 10 episodes)')\n    plt.grid(True, alpha=0.3)\n    plt.axhline(y=mean_reward_bound, color='r', linestyle='--', label=f'Target ({mean_reward_bound})')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(range(len(plot_rewards)), plot_rewards, 'g-', linewidth=2)\n    plt.title('Training Progress Detail')\n    plt.xlabel('Episodes (x10)')\n    plt.ylabel('Average Reward')\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    # Save plot to file\n    plot_path = os.path.join(artifacts_dir, f\"fc_training_plot_ep{episode}.png\")\n    plt.savefig(plot_path)\n    mlflow.log_artifact(plot_path)\n    wandb.log({\"training_plot\": wandb.Image(plot_path)})\n    plt.close()  # Close to save memory\n    \n    print(f\"Plot saved at episode {episode}: {plot_path}\")\n    return plot_path\n\ndef display_training_plot(episode_rewards, mean_reward_bound):\n    \"\"\"Display training plot to screen\"\"\"\n    if len(episode_rewards) <= 1:\n        return\n    \n    # Calculate mean rewards for plotting (using 10-episode averages)\n    plot_episodes = []\n    plot_rewards = []\n    for i in range(10, len(episode_rewards) + 1, 10):\n        plot_episodes.append(i)\n        plot_rewards.append(np.mean(episode_rewards[i-10:i]))\n    \n    clear_output(wait=True)\n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(plot_episodes, plot_rewards, 'b-', linewidth=2, marker='o')\n    plt.title('FC-DQN Pong Training Progress')\n    plt.xlabel('Episode')\n    plt.ylabel('Mean Reward (last 10 episodes)')\n    plt.grid(True, alpha=0.3)\n    plt.axhline(y=mean_reward_bound, color='r', linestyle='--', label=f'Target ({mean_reward_bound})')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(range(len(plot_rewards)), plot_rewards, 'g-', linewidth=2)\n    plt.title('Training Progress Detail')\n    plt.xlabel('Episodes (x10)')\n    plt.ylabel('Average Reward')\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\ndef log_final_results(model, episode, mean_reward, episode_rewards, artifacts_dir, run_id):\n    \"\"\"Log final model and metrics\"\"\"\n    # Log final model\n    mlflow.pytorch.log_model(model, \"final_model\")\n    \n    # Log final metrics\n    final_metrics = {\n        \"final_episode\": episode,\n        \"final_mean_reward\": mean_reward,\n        \"total_episodes\": len(episode_rewards)\n    }\n    \n    mlflow.log_metrics(final_metrics)\n    wandb.log(final_metrics)\n    \n    # Save final model to wandb (in run-specific folder)\n    final_model_path = os.path.join(artifacts_dir, 'final_pong_dqn_fc_model.pth')\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'run_id': run_id,\n        'final_metrics': final_metrics\n    }, final_model_path)\n    wandb.save(final_model_path)\n    \n    return final_model_path",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cmvskin03kg",
   "source": "class SimpleDQN(nn.Module):\n    def __init__(self, state_size, action_size):\n        super(SimpleDQN, self).__init__()\n        # Updated to use downscaled frame dimensions: 2 * 84 * 84 (2 frames at 84x84)\n        self.fc = nn.Sequential(\n            nn.Linear(2 * 84 * 84, 512),\n            nn.ReLU(),\n            nn.Linear(512, action_size)\n        )\n    \n    def forward(self, x):\n        # Flatten stacked frames: (batch, 2, 84, 84) -> (batch, 2*84*84)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n    \n    def push(self, state, action, reward, next_state, done):\n        self.buffer.append((state, action, reward, next_state, done))\n    \n    def sample(self, batch_size):\n        batch = random.sample(self.buffer, batch_size)\n        state, action, reward, next_state, done = zip(*batch)\n        return np.array(state), action, reward, np.array(next_state), done\n    \n    def __len__(self):\n        return len(self.buffer)\n\ndef preprocess_frame(frame):\n    \"\"\"Convert frame to grayscale, crop, and downscale to 84x84\"\"\"\n    import cv2\n    \n    # Convert to grayscale\n    gray = np.mean(frame, axis=2).astype(np.uint8)\n    \n    # Crop the frame to remove score area and focus on game area\n    # Original frame is 210x160, crop to 160x160 by removing top 34 pixels and bottom 16 pixels\n    cropped = gray[34:194, :]  # Keep rows 34-193 (160 rows total), all columns (160)\n    \n    # Downscale to 84x84 using OpenCV\n    resized = cv2.resize(cropped, (84, 84), interpolation=cv2.INTER_AREA)\n    \n    # Normalize to [0, 1]\n    return resized.astype(np.float32) / 255.0\n\nclass FrameStack:\n    \"\"\"Stack multiple frames for temporal information\"\"\"\n    def __init__(self, num_frames=2):\n        self.num_frames = num_frames\n        self.frames = deque(maxlen=num_frames)\n    \n    def reset(self, frame):\n        \"\"\"Reset with initial frame repeated num_frames times\"\"\"\n        processed_frame = preprocess_frame(frame)\n        for _ in range(self.num_frames):\n            self.frames.append(processed_frame)\n        return self.get_stacked()\n    \n    def step(self, frame):\n        \"\"\"Add new frame and return stacked frames\"\"\"\n        processed_frame = preprocess_frame(frame)\n        self.frames.append(processed_frame)\n        return self.get_stacked()\n    \n    def get_stacked(self):\n        \"\"\"Return stacked frames as numpy array with shape (2, 84, 84)\"\"\"\n        return np.stack(list(self.frames), axis=0)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "r9ogn9wl4i",
   "source": "import ale_py\ngym.register_envs(ale_py)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb567a85",
   "metadata": {},
   "outputs": [],
   "source": "def train_dqn():\n    # End any active runs first\n    if mlflow.active_run():\n        mlflow.end_run()\n    \n    # MLflow experiment setup\n    mlflow.set_experiment(\"Pong_FC_DQN_Training\")\n    \n    with mlflow.start_run():\n        # Create artifacts directory using MLflow run ID\n        run_id = mlflow.active_run().info.run_id\n        artifacts_dir = f\"/home/bmartins/dev/rl_study/artifacts/run_{run_id}\"\n        os.makedirs(artifacts_dir, exist_ok=True)\n        print(f\"Artifacts will be saved to: {artifacts_dir}\")\n        \n        # Environment setup\n        env = gym.make('Pong-v4')\n        \n        # Use global hyperparameters\n        action_size = env.action_space.n\n        lr = LEARNING_RATE\n        gamma = GAMMA\n        epsilon = EPSILON_START\n        epsilon_min = EPSILON_MIN\n        epsilon_decay = EPSILON_DECAY\n        batch_size = BATCH_SIZE\n        buffer_size = BUFFER_SIZE\n        target_update = TARGET_UPDATE\n        mean_reward_bound = MEAN_REWARD_BOUND\n        frame_skip = FRAME_SKIP\n        \n        # Log hyperparameters using external function\n        log_hyperparameters(artifacts_dir)\n        \n        # Initialize networks and buffer\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Using device: {device}\")\n        \n        main_net = SimpleDQN(None, action_size).to(device)\n        target_net = SimpleDQN(None, action_size).to(device)\n        target_net.load_state_dict(main_net.state_dict())\n        \n        # Log model info using external function\n        total_params = log_model_info(main_net, device)\n        \n        optimizer = optim.Adam(main_net.parameters(), lr=lr)\n        buffer = ReplayBuffer(buffer_size)\n        \n        # Training loop\n        episode_rewards = []\n        mean_rewards = []\n        episode = 0\n        \n        while True:\n            state, _ = env.reset()\n            frame_stack = FrameStack(FRAME_STACK)\n            stacked_state = frame_stack.reset(state)\n            total_reward = 0\n            done = False\n            step_count = 0\n            \n            while not done:\n                # Epsilon-greedy action selection\n                if random.random() > epsilon:\n                    with torch.no_grad():\n                        state_tensor = torch.FloatTensor(stacked_state).unsqueeze(0).to(device)\n                        q_values = main_net(state_tensor)\n                        action = q_values.max(1)[1].item()\n                else:\n                    action = env.action_space.sample()\n                \n                # Take action\n                next_state, reward, terminated, truncated, _ = env.step(action)\n                done = terminated or truncated\n                next_stacked_state = frame_stack.step(next_state)\n                \n                # Store experience\n                buffer.push(stacked_state, action, reward, next_stacked_state, done)\n                stacked_state = next_stacked_state\n                total_reward += reward\n                step_count += 1\n                \n                # Train if buffer has enough samples\n                if len(buffer) > batch_size:\n                    # Sample batch\n                    states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n                    \n                    # Convert to tensors\n                    states = torch.FloatTensor(states).to(device)\n                    actions = torch.LongTensor(actions).to(device)\n                    rewards = torch.FloatTensor(rewards).to(device)\n                    next_states = torch.FloatTensor(next_states).to(device)\n                    dones = torch.BoolTensor(dones).to(device)\n                    \n                    # Current Q values\n                    current_q_values = main_net(states).gather(1, actions.unsqueeze(1))\n                    \n                    # Next Q values\n                    with torch.no_grad():\n                        next_q_values = target_net(next_states).max(1)[0]\n                        target_q_values = rewards + (gamma * next_q_values * ~dones)\n                    \n                    # Compute loss\n                    loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n                    \n                    # Optimize\n                    optimizer.zero_grad()\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(main_net.parameters(), max_norm=10)\n                    optimizer.step()\n                    \n                    # Log loss using external function\n                    if step_count % 1000 == 0:\n                        log_training_step(loss.item(), episode, step_count)\n            \n            # Update target network\n            if episode % target_update == 0:\n                target_net.load_state_dict(main_net.state_dict())\n            \n            # Decay epsilon\n            if epsilon > epsilon_min:\n                epsilon *= epsilon_decay\n\n            # Check mean reward, not single episode\n            episode_rewards.append(total_reward)\n            mean_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n            \n            # Log episode metrics using external function\n            log_episode_metrics(total_reward, mean_reward, epsilon, len(buffer), step_count, episode)\n            \n            if mean_reward >= mean_reward_bound:\n                print(f\"Environment solved in {episode} episodes with mean reward: {mean_reward:.2f}\")\n                log_solved_episode(episode)\n                break\n            \n            # Save checkpoint every 1000 episodes\n            if episode > 0 and episode % 1000 == 0:\n                save_checkpoint(artifacts_dir, episode, main_net, optimizer, episode_rewards, epsilon, run_id)\n                save_training_plot(artifacts_dir, episode_rewards, episode, mean_reward_bound)\n            \n            # Display plot to screen every 100 episodes\n            if episode % 100 == 0 and episode > 0:\n                avg_reward = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)\n                mean_rewards.append(avg_reward)\n                print(f\"Episode {episode}, Avg Reward: {avg_reward:.2f}, Epsilon: {epsilon:.3f}, Buffer: {len(buffer)}\")\n                \n                display_training_plot(episode_rewards, mean_reward_bound)\n            \n            # Print progress every 10 episodes (no plotting)\n            elif episode % 10 == 0 and episode > 0:\n                avg_reward = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)\n                print(f\"Episode {episode}, Avg Reward: {avg_reward:.2f}, Epsilon: {epsilon:.3f}, Buffer: {len(buffer)}\")\n                \n                # Log 10-episode average using external function\n                log_10_episode_average(avg_reward, episode)\n\n            episode += 1\n        \n        # Log final results using external function\n        final_model_path = log_final_results(main_net, episode, mean_reward, episode_rewards, artifacts_dir, run_id)\n        \n        print(f\"Training completed! All artifacts saved in: {artifacts_dir}\")\n        \n        env.close()\n        wandb.finish()\n        return main_net"
  },
  {
   "cell_type": "code",
   "id": "k0pmurlfb4",
   "source": "# Test FC architecture\nenv = gym.make('Pong-v4')\nstate, _ = env.reset()\n\nprint(f\"Original frame shape: {state.shape}\")\n\nprocessed = preprocess_frame(state)\nprint(f\"Processed frame shape: {processed.shape}\")\n\nframe_stack = FrameStack(2)\nstacked = frame_stack.reset(state)\nprint(f\"Stacked frames shape: {stacked.shape}\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfc_net = SimpleDQN(None, env.action_space.n).to(device)\n\nwith torch.no_grad():\n    state_tensor = torch.FloatTensor(stacked).unsqueeze(0).to(device)\n    q_values = fc_net(state_tensor)\n    print(f\"Network input shape: {state_tensor.shape}\")\n    print(f\"Flattened input size: {state_tensor.view(1, -1).shape}\")\n    print(f\"Q-values shape: {q_values.shape}\")\n    print(f\"Q-values: {q_values.cpu().numpy()}\")\n\ntotal_params = sum(p.numel() for p in fc_net.parameters())\nprint(f\"\\nTotal parameters: {total_params:,}\")\n\n# Compare with CNN version\ncnn_params_approx = 3354278  # From CNN version\nprint(f\"CNN parameters (approx): {cnn_params_approx:,}\")\nprint(f\"FC vs CNN parameter ratio: {total_params / cnn_params_approx:.2f}x\")\n\nenv.close()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb5eaf7",
   "metadata": {},
   "outputs": [],
   "source": "# Ensure no active MLflow runs before training\nif mlflow.active_run():\n    print(\"Ending active MLflow run...\")\n    mlflow.end_run()\n\n# Train the FC-based DQN\ntrained_model = train_dqn()\nprint(\"FC-DQN Training completed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xazdl2bzwt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test frame cropping and downscaling\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create environment and get a sample frame\n",
    "env = gym.make('Pong-v4')\n",
    "state, _ = env.reset()\n",
    "\n",
    "print(f\"Original frame shape: {state.shape}\")\n",
    "\n",
    "# Test preprocessing\n",
    "processed = preprocess_frame(state)\n",
    "print(f\"Processed frame shape: {processed.shape}\")\n",
    "\n",
    "# Test frame stack\n",
    "frame_stack = FrameStack(2)  # Now using 2 frames\n",
    "stacked = frame_stack.reset(state)\n",
    "print(f\"Stacked frames shape: {stacked.shape}\")\n",
    "\n",
    "# Visualize the processing pipeline\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Original frame (convert to grayscale for display)\n",
    "original_gray = np.mean(state, axis=2).astype(np.uint8)\n",
    "ax1.imshow(original_gray, cmap='gray')\n",
    "ax1.set_title(f'Original Frame {original_gray.shape}')\n",
    "ax1.set_xlabel('Width (160 pixels)')\n",
    "ax1.set_ylabel('Height (210 pixels)')\n",
    "\n",
    "# Cropped frame (before downscaling)\n",
    "cropped = original_gray[34:194, :]\n",
    "ax2.imshow(cropped, cmap='gray')\n",
    "ax2.set_title(f'Cropped Frame {cropped.shape}')\n",
    "ax2.set_xlabel('Width (160 pixels)')\n",
    "ax2.set_ylabel('Height (160 pixels)')\n",
    "\n",
    "# Final processed frame\n",
    "ax3.imshow(processed, cmap='gray')\n",
    "ax3.set_title(f'Final Processed Frame {processed.shape}')\n",
    "ax3.set_xlabel('Width (84 pixels)')\n",
    "ax3.set_ylabel('Height (84 pixels)')\n",
    "\n",
    "# Show network input size comparison\n",
    "sizes = ['Original\\n(210×160×3)', 'Cropped\\n(160×160)', 'Downscaled\\n(84×84)', 'Network Input\\n(2×84×84)']\n",
    "params = [210*160*3, 160*160, 84*84, 2*84*84]\n",
    "colors = ['red', 'orange', 'yellow', 'green']\n",
    "\n",
    "ax4.bar(sizes, params, color=colors, alpha=0.7)\n",
    "ax4.set_title('Memory Usage Comparison')\n",
    "ax4.set_ylabel('Number of Values')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(params):\n",
    "    ax4.text(i, v + max(params)*0.01, f'{v:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMemory reduction:\")\n",
    "print(f\"Original: {210*160*3:,} values\")\n",
    "print(f\"Final: {2*84*84:,} values\")\n",
    "print(f\"Reduction factor: {(210*160*3)/(2*84*84):.1f}x smaller\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_study_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}